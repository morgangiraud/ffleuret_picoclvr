#!/usr/bin/env python

import math

import torch, torchvision

from torch import nn
from torch.nn import functional as F

######################################################################


class Problem:
    def generate_sequences(self, nb):
        pass

    def seq2str(self, seq):
        return "[NOT IMPLEMENTED]"

    def compute_nb_correct(self, input, ar_mask, result):
        nb_total = ar_mask.sum().item()
        nb_correct = ((result == input).long() * ar_mask).sum().item()
        return nb_total, nb_correct


####################


class ProblemDegradation(Problem):
    def __init__(self, nb_state_tokens=5, nb_time_steps=12, value_max=25, hard=False):
        assert value_max // nb_state_tokens >= 2
        self.nb_state_tokens = nb_state_tokens
        self.nb_time_steps = nb_time_steps
        self.value_max = value_max
        self.hard = hard

    def generate_sequences(self, nb):
        x = (
            torch.rand(nb, self.nb_state_tokens).sort(dim=-1).indices == 0
        ).long() * self.value_max
        seq = [x]

        for t in range(self.nb_time_steps - 1):
            v = (torch.rand(x.size()).sort(dim=-1).indices + 1) * (x >= 2).long()
            u = (v.max(dim=-1, keepdim=True).values == v).long()
            n = (
                (u * x)
                .minimum(2 + torch.randint(self.value_max // 4 - 2, x.size()))
                .sum(dim=-1, keepdim=True)
            )
            m = 1 + ((n - 1) * torch.rand(n.size())).long()
            x = (
                x
                + m * u.roll(shifts=-1, dims=-1)
                - n * u
                + (n - m) * u.roll(shifts=1, dims=-1)
            )
            seq.append(x)

        if self.hard:
            seq.reverse()

        seq = torch.cat(seq, dim=1)
        return seq, seq.new_full(seq.size(), 1, dtype=torch.int64)

    def compute_nb_correct(self, input, ar_mask, result):
        nb_total = result.size(0)
        nb_correct = 0
        e = result.new_zeros(self.nb_state_tokens)

        for seq in result:
            states = list(seq.split(self.nb_state_tokens))
            if self.hard:
                states.reverse()

            d = states[0]
            j = d.sort(descending=True).indices[0]
            e.zero_()
            e[j] = self.value_max
            if (d - e).abs().sum() == 0:
                nb_errors = 0
                for k in range(len(states) - 1):
                    d = states[k + 1] - states[k]
                    j = d.sort(descending=False).indices[0]
                    if (
                        d[j] == 0
                        or d[j] > self.value_max // 4
                        or d[(j + 1) % e.size(0)] <= 0
                        or d[(j + 1) % e.size(0)] >= -d[j]
                    ):
                        nb_errors += 1
                    else:
                        e.zero_()
                        e[j] = d[j]
                        e[(j + 1) % e.size(0)] = d[(j + 1) % e.size(0)]
                        e[(j - 1) % e.size(0)] = -d[(j + 1) % e.size(0)] - d[j]
                        if (d - e).abs().sum() > 0:
                            nb_errors += 1
                if nb_errors == 0:
                    nb_correct += 1

        return nb_total, nb_correct

    def seq2str(self, seq):
        return " | ".join(
            [" ".join([f"{x:02d}" for x in s]) for s in seq.split(self.nb_state_tokens)]
        )


####################


class ProblemTwoTargets(Problem):
    def __init__(self, len_total=10, len_targets=3):
        assert len_targets >= 3
        assert len_total >= 3 * len_targets - 1
        self.len_total = len_total
        self.len_targets = len_targets

    def generate_sequences(self, nb):
        k = torch.arange(self.len_total)[None, :]
        s = torch.randint(10, (nb, self.len_total))
        l = torch.rand(nb, self.len_total)
        l = l * (k <= self.len_total - self.len_targets).long()
        k1 = l.argmax(dim=1, keepdim=True)
        m = (k != k1).long() * (k != k1 + self.len_targets - 1).long()
        s = s * m + 10 * (1 - m)
        l = l * (
            1
            - (k + self.len_targets - 1 >= k1).long()
            * (k < k1 + self.len_targets).long()
        )
        k2 = l.argmax(dim=1, keepdim=True)
        m = (k != k2).long() * (k != k2 + self.len_targets - 1).long()
        s = s * m + 11 * (1 - m)
        a1 = s.gather(dim=1, index=k1 + 1 + torch.arange(self.len_targets - 2)[None, :])
        a2 = s.gather(dim=1, index=k2 + 1 + torch.arange(self.len_targets - 2)[None, :])
        sequences = torch.cat(
            (
                s,
                torch.full((nb, 1), 12),
                a1,
                torch.full((nb, 1), 12),
                a2,
                torch.full((nb, 1), 12),
            ),
            1,
        )
        ar_mask = (sequences == 12).long()
        ar_mask = (ar_mask.cumsum(1) - ar_mask).clamp(max=1)
        return sequences, ar_mask

    def seq2str(self, seq):
        return "".join("0123456789-+|"[x.item()] for x in seq)


####################


class ProblemByHeart(Problem):
    def __init__(self, nb_sentences=100, len_prompt=8, len_result=8):
        self.seq = torch.randint(10, (nb_sentences, len_prompt + 1 + len_result))
        self.seq[:, len_prompt] = 10

    def generate_sequences(self, nb):
        sequences = self.seq[torch.randint(self.seq.size(0), (nb,))]
        ar_mask = (sequences == 10).long()
        ar_mask = (ar_mask.cumsum(1) - ar_mask).clamp(max=1)
        return sequences, ar_mask

    def seq2str(self, seq):
        return "".join("0123456789|"[x.item()] for x in seq)


####################


class ProblemLearnOperator(Problem):
    def __init__(self, nb_operators=100, len_source=6, len_result=9):
        self.len_source = len_source
        self.len_result = len_result
        self.len_nb_operator = int(math.log(nb_operators) / math.log(10)) + 1
        self.operators = F.one_hot(
            torch.rand(nb_operators, len_result, len_source).argmax(-1),
            num_classes=len_source,
        )

    def generate_sequences(self, nb):
        nb_operators = torch.randint(self.operators.size(0), (nb,))
        operators = self.operators[nb_operators]
        nb_operators = (
            nb_operators[:, None]
            // 10 ** torch.arange(self.len_nb_operator - 1, -1, -1)
        ) % 10
        marker1 = torch.full((nb, 1), 10)
        source = torch.rand(nb, 10).sort(dim=1).indices[:, : self.len_source]
        marker2 = torch.full((nb, 1), 11)
        result = operators.bmm(source[:, :, None]).squeeze(-1)
        sequences = torch.cat((nb_operators, marker1, source, marker2, result), 1)
        ar_mask = (sequences == 11).long()
        ar_mask = (ar_mask.cumsum(1) - ar_mask).clamp(max=1)
        return sequences, ar_mask

    def seq2str(self, seq):
        return "".join("0123456789|>"[x.item()] for x in seq)


####################


class ProblemGuessOperator(Problem):
    def __init__(self, len_source=5, len_result=8):
        self.len_source = len_source
        self.len_result = len_result

    def generate_sequences(self, nb):
        operators = F.one_hot(
            torch.rand(nb, self.len_result, self.len_source).argmax(-1),
            num_classes=self.len_source,
        )
        source1 = torch.rand(nb, 10).sort(dim=1).indices[:, : self.len_source]
        marker1 = torch.full((nb, 1), 10)
        result1 = operators.bmm(source1[:, :, None]).squeeze(-1)
        marker2 = torch.full((nb, 1), 11)
        source2 = torch.randint(10, (nb, self.len_source))
        marker3 = torch.full((nb, 1), 12)
        result2 = operators.bmm(source2[:, :, None]).squeeze(-1)

        sequences = torch.cat(
            (source1, marker1, result1, marker2, source2, marker3, result2), 1
        )
        ar_mask = (sequences == 12).long()
        ar_mask = (ar_mask.cumsum(1) - ar_mask).clamp(max=1)
        return sequences, ar_mask

    def seq2str(self, seq):
        return "".join("0123456789>|~"[x.item()] for x in seq)


####################


class ProblemAddition(Problem):
    def __init__(self, nb_digits=10, zero_padded=False, inverted_result=False):
        self.nb_digits = nb_digits
        self.zero_padded = zero_padded
        self.inverted_result = inverted_result
        self.char2id = dict([(c, n) for n, c in enumerate("0123456789+=$")])
        self.id2char = dict([(n, c) for c, n in self.char2id.items()])

    def tensorize(self, strings):
        len_max = max([len(x) for x in strings])
        return torch.cat(
            [
                torch.tensor(
                    [
                        [self.char2id[c] for c in s + "$" * (len_max - len(s))]
                        for s in strings
                    ]
                )
            ],
            0,
        )

    def generate_sequences(self, nb):
        sequences = []
        for k in range(nb):
            a, b = torch.randint(10**self.nb_digits, (2,))
            c = a + b
            a, b, c = str(a.item()), str(b.item()), str(c.item())
            if self.zero_padded:
                a = "0" * (self.nb_digits - len(a)) + a
                b = "0" * (self.nb_digits - len(b)) + b
                c = "0" * (self.nb_digits + 1 - len(c)) + c
            if self.inverted_result:
                c = c[::-1]
            sequences.append(f"{a}+{b}={c}$")

        sequences = self.tensorize(sequences)
        ar_mask = (sequences == self.char2id["="]).long()
        ar_mask = (ar_mask.cumsum(1) - ar_mask).clamp(max=1)
        return sequences, ar_mask

    def seq2str(self, seq):
        return "".join(self.id2char[x.item()] for x in seq)


####################


class ProblemMixing(Problem):
    def __init__(
        self, height=4, width=4, nb_time_steps=9, hard=False, random_start=True
    ):
        self.height = height
        self.width = width
        self.nb_time_steps = nb_time_steps
        self.hard = hard
        self.random_start = random_start

    def start_random(self, nb):
        y = torch.arange(self.height * self.width).reshape(1, -1).expand(nb, -1)

        if self.random_start:
            i = (
                torch.arange(self.height)
                .reshape(1, -1, 1)
                .expand(nb, self.height, self.width)
            )
            j = (
                torch.arange(self.width)
                .reshape(1, 1, -1)
                .expand(nb, self.height, self.width)
            )

            ri = torch.randint(self.height, (nb,)).reshape(nb, 1, 1)
            rj = torch.randint(self.width, (nb,)).reshape(nb, 1, 1)

            m = 1 - torch.logical_or(i == ri, j == rj).long().flatten(1)

            y = y * m + self.height * self.width * (1 - m)

        y = y.reshape(nb, self.height, self.width)

        return y

    def start_error(self, x):
        i = torch.arange(self.height, device=x.device).reshape(1, -1, 1).expand_as(x)
        j = torch.arange(self.width, device=x.device).reshape(1, 1, -1).expand_as(x)

        ri = (
            (x == self.height * self.width).long().sum(dim=-1).argmax(-1).view(-1, 1, 1)
        )
        rj = (
            (x == self.height * self.width).long().sum(dim=-2).argmax(-1).view(-1, 1, 1)
        )

        m = 1 - torch.logical_or(i == ri, j == rj).long().flatten(1)

        x = x.flatten(1)
        u = torch.arange(self.height * self.width, device=x.device).reshape(1, -1)

        d = (x - (m * u + (1 - m) * self.height * self.width)).abs().sum(-1)
        return d

    def moves(self, x):
        y = (
            x[:, None, :, :]
            .expand(-1, self.height * 2 + self.width * 2, -1, -1)
            .clone()
        )
        k = 0

        for i in range(self.height):
            y[:, k, i, :] = y[:, k, i, :].roll(dims=-1, shifts=-1)
            k += 1
            y[:, k, i, :] = y[:, k, i, :].roll(dims=-1, shifts=1)
            k += 1

        for j in range(self.width):
            y[:, k, :, j] = y[:, k, :, j].roll(dims=-1, shifts=-1)
            k += 1
            y[:, k, :, j] = y[:, k, :, j].roll(dims=-1, shifts=1)
            k += 1

        return y

    def generate_sequences(self, nb):
        x = self.start_random(nb)

        seq = [x.flatten(1)]

        for t in range(self.nb_time_steps - 1):
            y = self.moves(x)
            x = y[torch.arange(nb), torch.randint(y.size(1), (nb,))]
            seq.append(x.flatten(1))

        if self.hard:
            seq.reverse()

        seq = torch.cat(seq, dim=1)
        return seq, seq.new_full(seq.size(), 1, dtype=torch.int64)

    def compute_nb_correct(self, input, ar_mask, result):
        a = [
            x.reshape(result.size(0), self.height, self.width)
            for x in result.split(self.height * self.width, dim=1)
        ]
        if self.hard:
            a.reverse()

        x = a[0]

        d = self.start_error(x)

        for t in range(self.nb_time_steps - 1):
            x0, x = a[t], a[t + 1]
            y = self.moves(x0)
            d = d + (x[:, None] - y).abs().sum((-1, -2)).min(dim=-1).values

        nb_total, nb_correct = result.size(0), (d == 0).long().sum().item()

        return nb_total, nb_correct

    def seq2str(self, seq):
        return " | ".join(
            [
                " ".join(
                    [
                        "-".join(
                            [
                                f"{x:02d}" if x < self.height * self.width else "**"
                                for x in s
                            ]
                        )
                        for s in r.split(self.width)
                    ]
                )
                for r in seq.split(self.height * self.width)
            ]
        )


####################

if __name__ == "__main__":
    p = ProblemMixing()
    s, m = p.generate_sequences(10000)
    for x in s[:5]:
        print(p.seq2str(x))
    print(p.compute_nb_correct(None, None, s))
